{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import matplotlib as mp;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "import math;\n",
    "import scipy.special;\n",
    "from sklearn.neighbors import KernelDensity;\n",
    "from sklearn.model_selection import GridSearchCV;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical CDF and PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical CDF at a given $x$ can be simply obtained by taking the fraction of data points falling below $x$:\n",
    "\\begin{equation}\n",
    "\\widehat{F}(x) = \\frac{1}{n} \\left| \\{x_i \\mid x_i < x \\} \\right|. \\nonumber\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a random sample distributed according to the Normal distribution and compare the empirical and the true CDF, which can be written as \n",
    "\\begin{equation}\n",
    "F(x) = \\frac{1}{2}\\left[1+{\\rm erf}\\left(\\frac{x - \\mu}{\\sqrt{2}\\sigma}\\right)\\right]. \\nonumber\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 2.0;\n",
    "sigma = 5.0;\n",
    "num_data_points = 5000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = np.random.normal(mu,sigma,num_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the empirical CDF, we can e.g., make first a histogram, and then use numpy.cumsum() that returns a list with the elements equal to the summed values up to the given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts,bins = np.histogram(norm_data,bins=50,density=False); # it is important that here we simply count \n",
    "                                                             # the number of data points in the bins, and do \n",
    "                                                             # not normalise.\n",
    "counts = counts.astype(float)/len(norm_data); # here we divide the counts by n, the number of data points.               \n",
    "cdf = np.cumsum(counts);   # and finally, this is where add up the results up to a given bin.\n",
    "\n",
    "plt.clf();\n",
    "plt.plot(bins[:-1],cdf, label = 'empirical CDF');\n",
    "\n",
    "x_vec = np.arange(mu - 3.0*sigma,mu + 3.0*sigma,0.1);\n",
    "y_vec = [0.5*(1.0+scipy.special.erf((x-mu)/math.sqrt(2.0)/sigma)) for x in x_vec];\n",
    "plt.plot(x_vec,y_vec, label = 'theoretical CDF');\n",
    "plt.ylim(-0.1,1.1);\n",
    "plt.legend(loc='upper left',fontsize = 10);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also avoid binning the data, and provide an empirical CDF \"as detailed as possible\". We have to simply sort the data point, and the empirical CDF is a step-wise increasing function, where the increase in the height is $1/n$ at a step.\n",
    "\n",
    "We can e.g., define a function that returns the sorted data points and the step-wise increasing CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDF(data_list):\n",
    "    num_points = len(data_list);\n",
    "    sorted_data = np.sort(data_list);\n",
    "    cdf = np.arange(1, num_points + 1) / num_points;\n",
    "    return sorted_data,cdf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_x,cdf_y = CDF(norm_data);\n",
    "plt.clf();\n",
    "plt.plot(cdf_x,cdf_y,label = 'empirical CDF');\n",
    "plt.plot(x_vec,y_vec, label = 'theoretical CDF');\n",
    "plt.ylim(-0.1,1.1);\n",
    "plt.legend(loc='upper left',fontsize = 10);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most of the discrepancy between the empirical and the true CDF in the previous figure was due to binning. \n",
    "\n",
    "Let's also prepare a **confidence band** based on the DKW inequality, which states that for a chosen $\\alpha$, a confidence band of the form \n",
    "\\begin{equation}\n",
    " P\\left(\\widehat{F}(x)-\\epsilon < F(x) <\\widehat{F}(x)+\\epsilon\\right)\\geq 1-\\alpha \\nonumber\n",
    "\\end{equation}\n",
    "can be given where \n",
    "\\begin{equation}\n",
    " \\epsilon= \\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)}. \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "We define a function that prepares both the empirical CDF and the confidence band for a chosen $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDF_confidence_band(data_list,alpha):\n",
    "    num_points = len(data_list);\n",
    "    sorted_data = np.sort(data_list);\n",
    "    cdf = np.arange(1, num_points + 1) / num_points;\n",
    "    eps = math.sqrt(0.5*math.log(2.0/alpha)/num_points);\n",
    "    L_x,U_x = [y + eps for y in cdf],[y - eps for y in cdf];\n",
    "    return {'x':sorted_data,'CDF':cdf,'L':L_x,'U':U_x};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_band = CDF_confidence_band(norm_data,alpha=0.0005);\n",
    "\n",
    "plt.clf();\n",
    "plt.fill_between(conf_band['x'],conf_band['U'],conf_band['L'],alpha = 0.2,color = 'red',label = 'confidence band');\n",
    "plt.plot(conf_band['x'],conf_band['CDF'], color = 'r',label = 'empirical CDF');\n",
    "plt.plot(x_vec,y_vec, label = 'theoretical CDF');\n",
    "plt.ylim(-0.1,1.1);\n",
    "plt.legend(loc='upper left',fontsize = 10);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to see how does the confidence band depend on the parameters.\n",
    " - ** try changing $\\alpha$ **,\n",
    " - ** try changing the number of data points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most simple estimator of the PDF $\\rho(x)$ is the histogram estimator with equal bin width $h$, where \n",
    "\\begin{equation}\n",
    "\\widehat{\\rho}_n(x)= \\sum_{j=1}^m \\frac{\\widehat{p}_j}{h} I(x \\in B_j)=\n",
    "\\sum_{j=1}^m \\frac{\\nu_j}{nh} I(x \\in B_j). \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Choosing the optimal bin width is a non-trivial problem. It turns out, that the mean integrated squared error, also called as risk in a cross validation approach can be estimated (apart from an additive constant independent of $h$) using\n",
    "\\begin{equation}\n",
    "\\widehat{J}(h)=\\int \\widehat{\\rho}_n^2(x)dx -\\frac{2}{n}\\sum_{i=1}^n \\widehat{\\rho}_{(i)}(x= x_i), \\nonumber\n",
    "\\end{equation}\n",
    "where $\\widehat{\\rho}_{(i)}$ is the histogram estimator obtained by removing data point $x_i$ from the sample. The value of $\\widehat{J}(h)$ can also be given in even more simple terms as\n",
    "\\begin{equation}\n",
    "\\widehat{J}(h)=\\frac{2}{h(n-1)} -\\frac{n+1}{h(n-1)}\\sum_{j=1}^m \\widehat{p}_j^2=\n",
    "\\frac{2}{h(n-1)} -\\frac{n+1}{h(n-1)n^2}\\sum_{j=1}^m \\nu_j^2. \\nonumber\n",
    "\\end{equation} \n",
    "\n",
    "Let's first prepare the histogram of the normal data set where we can adjust the bin width to any chosen value. \n",
    "\n",
    "We are going to use numpy's histogram function, and specify equal sized bins starting from the lowest sample element, ending at the highest sample value, and having a prescribed bin width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width = 0.8;\n",
    "n_hist,n_bins = np.histogram(norm_data,\n",
    "                             bins=np.arange(min(norm_data)-bin_width*0.5,max(norm_data)+bin_width*0.5,bin_width),\n",
    "                             normed = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram and the true PDF together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf();\n",
    "plt.step(n_bins[:-1],n_hist, where = 'post', label = 'histogram, h='+str(bin_width));\n",
    "pdf_theor = [math.exp(-(x-mu)**2/2.0/sigma**2)/math.sqrt(2.0*np.pi)/sigma for x in n_bins[:-1]];\n",
    "plt.plot(n_bins[:-1],pdf_theor,label= 'true PDF');\n",
    "plt.legend(loc = 'upper right',fontsize = 10)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Try out smaller and larger bin width values and see how the histogram is changing **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now examine the estimate for the risk in order to be able to choose the optimal bin width.\n",
    "\n",
    "First, we define a function calculating $\\widehat{J}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_estimate(data,bin_width):\n",
    "    counts,bins = np.histogram(data,\n",
    "                             bins=np.arange(min(data)-bin_width*0.5,max(data)+bin_width*0.5,bin_width),\n",
    "                             normed = False);\n",
    "    n_d = len(data);\n",
    "    p_list = [c/n_d for c in counts];\n",
    "    return 2.0/bin_width/(n_d-1.0)-(n_d + 1.0)/bin_width/(n_d-1.0)*sum([p*p for p in p_list]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate it for a range of $h$  bin width values, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = np.arange(0.01,5,0.05);\n",
    "J_list = [J_estimate(norm_data,h) for h in h_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf();\n",
    "plt.plot(h_list,J_list);\n",
    "plt.xlabel('$h$')\n",
    "plt.ylabel('$\\widehat{J}(h)$')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the optimal bin width is somewhere around 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF for real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the framework shown above for real data. We are going to use the methylation data from the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"ASC_cg_export.tsv\";\n",
    "pd.read_csv(file_name, sep = '\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above just showed the data, let's now read it into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(file_name,sep = '\\t',low_memory=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's determine the optimal bin width using $\\widehat{J}(h)$. Important: **the data frame contains nan values in some places**, which can ruin the analysis, so we have to get rid of them before actually doing any calculations. A simple solution is to convert the columns to numpy arrays and remove the nans from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_h_range = np.arange(0.0001,0.1,0.0001);  # this is the range for the h binwidth values\n",
    "\n",
    "plt.clf();\n",
    "for cg_ind in range(1,5):\n",
    "    data_with_possible_nans = np.array(data_frame.iloc[:,cg_ind]); # at this phase we can have possibly nans  \n",
    "    data_without_nans = data_with_possible_nans[np.logical_not(np.isnan(data_with_possible_nans))]; # here we get rid\n",
    "                                                                                                    # of them\n",
    "    \n",
    "    J_cg_list = [J_estimate(data_without_nans,b) for b in cg_h_range];   # we simply apply the J estimator\n",
    "    plt.plot(cg_h_range,J_cg_list,label = data_frame.columns[cg_ind]);   # and plot the results\n",
    "#plt.xlim(0,0.02);\n",
    "plt.xlabel('bin width');\n",
    "plt.legend(loc = 'upper right',fontsize = 10);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that $h=0.008$ is a reasonably good choice for the bin width.\n",
    "\n",
    "Let's now prepare the histogram estimator for the PDFs and plot them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width = 0.008;\n",
    "plt.clf();\n",
    "for cg_ind in range(1,5):\n",
    "    data_with_possible_nans = np.array(data_frame.iloc[:,cg_ind]);\n",
    "    data_without_nans = data_with_possible_nans[np.logical_not(np.isnan(data_with_possible_nans))];\n",
    "    cg_hist,cg_bins = np.histogram(data_without_nans,\n",
    "                                   bins = np.arange(min(data_without_nans)-bin_width*0.5,\n",
    "                                                    max(data_without_nans)+bin_width*0.5,bin_width),\n",
    "                                   normed = True);\n",
    "    plt.step(cg_bins[:-1],cg_hist,where = 'post',label = data_frame.columns[cg_ind]);\n",
    "plt.xlim(-bin_width*0.5,1.0+bin_width);\n",
    "plt.legend(loc = 'upper right',fontsize = 10);\n",
    "plt.xlabel('methylation level');\n",
    "plt.ylabel('PDF');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel density estimation of the PDF is based on some smooth kernel function $K(x)$ with 0 mean and positive variance, and a bandwidth $h$. At a given point $x$, the estimate for $\\rho(x)$ is written as\n",
    "\\begin{equation}\n",
    "\\widehat{\\rho}_n(x)=\\frac{1}{n}\\sum_{i=1}^n\\frac{1}{h}K\\left(\\frac{x-x_i}{h}\\right). \\nonumber\n",
    "\\end{equation}\n",
    "The advantages of this approach compared to the histogram estimator are that\n",
    " - the obtained empirical PDF is much smoother,\n",
    " - and it also has faster convergence to the true PDF as a function of $n$ if we choose the optimal bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!\n",
    "\n",
    "First we define the estimator and the kernel function, which, for simplicity is going to be a Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kernel_estimate(Kernel_func,data,x,bandwidth):\n",
    "    return sum([Kernel_func((x - x_i)/bandwidth) for x_i in data])/bandwidth/len(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_f(x):\n",
    "    return math.exp(-0.5*x*x)/math.sqrt(2.0*np.pi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this e.g., to the normal data we have generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_bw = 15.2;                      # the bandwidth \n",
    "x_list = np.arange(-15,20,0.1);  # the x values where we evaluate the estimator;\n",
    "y_list = [Kernel_estimate(K_f,norm_data,x,h_bw) for x in x_list];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot the result together with the true PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf();\n",
    "plt.plot(x_list,y_list,label = 'Kernel density estimator');\n",
    "pdf_th = [math.exp(-(x-mu)**2/2.0/sigma**2)/math.sqrt(2.0*np.pi)/sigma for x in x_list];\n",
    "plt.plot(x_list,pdf_th,label= 'true PDF');\n",
    "plt.legend(loc = 'lower right',fontsize = 10);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Try out a few bandwidth values and see what happens. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to determine the optimal value for the bandwidth?\n",
    "\n",
    "Luckily, kernel density estimation is already implemented in **sklearn**, and we can use the functions defined there.\n",
    "\n",
    "First we need to 'reshape' our data to make it compatible with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_reshaped = norm_data[:,np.newaxis];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's determine the optimal bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bandwidth': np.linspace(1.0, 2.0, 10)}\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'), params)\n",
    "grid.fit(norm_data_reshaped);\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that, we can prepare the kernel density estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_kde = KernelDensity(kernel='gaussian', bandwidth=1.56).fit(norm_data_reshaped);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshaped = np.linspace(-15, 20, 1000)[:, np.newaxis];\n",
    "log_dens = norm_kde.score_samples(x_reshaped);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf();\n",
    "plt.plot(x_reshaped,np.exp(log_dens),label = 'Kernel density estimator');\n",
    "pdf_th = [math.exp(-(x-mu)**2/2.0/sigma**2)/math.sqrt(2.0*np.pi)/sigma for x in x_list];\n",
    "plt.plot(x_list,pdf_th,label= 'true PDF');\n",
    "plt.legend(loc = 'lower right',fontsize = 10);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's prepare the kernel density estimator for the PDF of one of the methylation positions.\n",
    "\n",
    "First, we need to get rid of the nans and put the data into a shape that can be processed further by the sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meth_data_with_nans = np.array(data_frame.iloc[:,3]);\n",
    "meth_data = meth_data_with_nans[np.logical_not(np.isnan(meth_data_with_nans))];\n",
    "meth_data = meth_data[:,np.newaxis];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we determine the optimal bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bandwidth': np.linspace(0.001, 0.02, 200)}\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'), params)\n",
    "grid.fit(meth_data);\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now prepare the KDE, and evaluate the resulting empirical PDF in a range of x values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_methyl = KernelDensity(kernel='gaussian', bandwidth=0.0174).fit(meth_data);\n",
    "meth_x_list = np.linspace(-0.01, 1.01, 1000)[:, np.newaxis];\n",
    "meth_log_dens = kde_methyl.score_samples(meth_x_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can prepare a plot, together with the former histogram estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_hist,cg_bins = np.histogram(meth_data,\n",
    "                               bins = np.arange(min(meth_data)-bin_width*0.5,max(meth_data)+bin_width*0.5,bin_width),\n",
    "                               normed = True);\n",
    "\n",
    "plt.clf();\n",
    "plt.plot(meth_x_list,np.exp(meth_log_dens),label='KDE estimator');\n",
    "plt.step(cg_bins[:-1],cg_hist,where='post',label='histogram estimator');\n",
    "plt.legend(loc='upper left', fontsize = 10);\n",
    "plt.xlabel('methylation level');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
